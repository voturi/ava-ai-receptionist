"""
Streaming AI Service for real-time LLM responses.

Uses OpenAI's streaming API to yield tokens as they're generated,
enabling low-latency text-to-speech synthesis.
"""

from __future__ import annotations

import os
from typing import Any, AsyncGenerator, Callable, Optional
import json

from openai import AsyncOpenAI


class StreamingAIService:
    """
    AI service with streaming token output.

    Designed for the streaming voice pipeline:
    STT transcript → LLM (streaming) → TTS (streaming)
    """

    def __init__(self):
        self.client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        # Use gpt-4o-mini for faster responses (good balance of speed/quality)
        self.model = "gpt-4o-mini"

    def get_system_prompt(
        self,
        business_name: str = "our business",
        business_config: Optional[dict] = None,
    ) -> str:
        """
        System prompt optimized for voice conversations.

        Key differences from text-based prompts:
        - Shorter responses (voice needs to be concise)
        - Natural speech patterns
        - No markdown or formatting
        """
        if business_config is None:
            business_config = {}

        ai_config = business_config.get("ai_config") or {}
        industry = business_config.get("industry") or "business"
        services = business_config.get("services") or []
        working_hours = business_config.get("working_hours") or {}

        tone = ai_config.get("tone", "warm, friendly, and professional")
        language = ai_config.get("language", "en-AU")

        services_summary = ", ".join(
            service.get("name", str(service)) if isinstance(service, dict) else str(service)
            for service in services[:8]
        )
        if not services_summary:
            services_summary = "Ask if the caller needs a service."

        working_hours_summary = ", ".join(
            f"{day}: {hours}" for day, hours in list(working_hours.items())[:7]
        )
        if not working_hours_summary:
            working_hours_summary = "Ask if the caller needs business hours."

        return f"""You are Echo, the AI receptionist for {business_name} ({industry}).

TONE AND LANGUAGE:
- Tone: {tone}
- Language: {language}
- Keep responses SHORT: 1-2 sentences, 15-25 words max
- Sound natural and warm, like a friendly human
- Never use bullet points, lists, or formatted text
- Don't say "I'm an AI" - just be helpful

BUSINESS CONTEXT:
- Services: {services_summary}
- Working hours: {working_hours_summary}

TOOLS:
- Use tools only when you need fresh, tenant-specific data.
- Max 2 tool calls per user turn.
- If a tool fails or times out, reply with:
  "I'm having trouble pulling that up right now. Would you like me to take a message?"
- Policies/FAQs require a topic string.
- Suggested policy/FAQ topics: cancellation, reschedule, late arrival, deposits,
  refunds, pricing, hours, location, parking.
- Booking lookups must use customer_phone from the call.
- If you need a tool, call it BEFORE responding.

VOICE CONVERSATION RULES:
- Use Australian expressions: "no worries", "lovely", "arvo"

BOOKING FLOW (one step at a time, don't skip any):
1. What service do they need?
2. What day works for them?
3. What time?
4. What's their name?
5. What's their mobile number? (for SMS confirmation)
6. Repeat back ALL details and confirm

IMPORTANT: You MUST ask for their mobile number before confirming. Never skip this step.

EXAMPLE CONVERSATION:
Customer: "I need a haircut"
You: "No worries! When would suit you - this week or next?"
Customer: "Wednesday arvo"
You: "Lovely! What time works - 2, 3, or 4pm?"
Customer: "2pm"
You: "Perfect! And your name?"
Customer: "Sarah"
You: "Great Sarah! What's your mobile for the SMS confirmation?"
Customer: "0412 345 678"
You: "Brilliant! So that's a haircut Wednesday 2pm for Sarah. I'll SMS you at 0412 345 678. All sorted!"

ENDING THE CALL:
- When customer says "thank you", "thanks", "bye", "that's all", or similar after booking is confirmed
- Say a brief friendly goodbye like "No worries! See you Wednesday. Bye!"
- End with the word "Goodbye" to signal call end

If unsure about anything, say "Let me check on that for you" and keep it brief."""

    async def get_streaming_response(
        self,
        user_message: str,
        conversation_history: Optional[list] = None,
        business_name: str = "our business",
    ) -> AsyncGenerator[str, None]:
        """
        Stream AI response token by token.

        Yields text chunks as they're generated by the LLM.
        Perfect for piping directly to streaming TTS.

        Usage:
            async for chunk in ai.get_streaming_response("Hello"):
                await tts.send_text(chunk)
            await tts.flush()
        """
        if conversation_history is None:
            conversation_history = []

        messages = [
            {"role": "system", "content": self.get_system_prompt(business_name)},
            *conversation_history,
            {"role": "user", "content": user_message},
        ]

        try:
            stream = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                max_tokens=100,  # Keep responses short for voice
                temperature=0.7,
                stream=True,
            )

            async for chunk in stream:
                delta = chunk.choices[0].delta
                if delta.content:
                    yield delta.content

        except Exception as e:
            print(f"❌ OpenAI Streaming Error: {e}")
            # Yield a fallback message
            yield "Sorry, I'm having trouble right now. Can you try again?"

    async def get_response_with_buffer(
        self,
        user_message: str,
        conversation_history: Optional[list] = None,
        business_name: str = "our business",
        min_chunk_size: int = 10,
    ) -> AsyncGenerator[str, None]:
        """
        Stream AI response with buffering for natural speech breaks.

        Buffers tokens until we hit a natural break point (punctuation)
        or reach min_chunk_size. This produces better TTS output.

        Args:
            user_message: The user's input
            conversation_history: Previous messages
            business_name: Business name for personalization
            min_chunk_size: Minimum characters before yielding on punctuation
        """
        buffer = ""

        async for token in self.get_streaming_response(
            user_message,
            conversation_history,
            business_name,
        ):
            buffer += token

            # Check for natural break points
            if self._should_yield(buffer, min_chunk_size):
                yield buffer
                buffer = ""

        # Yield any remaining text
        if buffer:
            yield buffer

    def _should_yield(self, buffer: str, min_size: int) -> bool:
        """
        Determine if we should yield the buffer to TTS.

        Yields on:
        - Sentence endings (. ! ?)
        - Clause breaks (, ; :) if buffer is long enough
        - Buffer exceeds max size (50 chars)
        """
        if not buffer:
            return False

        stripped = buffer.rstrip()

        # Always yield on sentence end
        if stripped.endswith((".", "!", "?")):
            return len(buffer) >= min_size

        # Yield on clause breaks if buffer is decent size
        if stripped.endswith((",", ";", ":")):
            return len(buffer) >= min_size

        # Yield if buffer is getting too long
        return len(buffer) >= 50

    async def stream_with_tools(
        self,
        user_message: str,
        conversation_history: Optional[list] = None,
        business_profile: Optional[dict] = None,
        tools: Optional[list] = None,
        tool_executor: Optional[Callable[[str, dict], Any]] = None,
        max_tool_calls: int = 2,
        prefetched_tools: Optional[list[dict]] = None,
    ) -> AsyncGenerator[dict, None]:
        """
        Stream a response with tool calling.

        Yields events:
        - {"type": "content", "text": "..."}
        - {"type": "tool_call", "name": "...", "arguments": {...}}
        """
        if conversation_history is None:
            conversation_history = []

        system_prompt = self.get_system_prompt(
            business_name=(business_profile or {}).get("business_name", "our business"),
            business_config=business_profile or {},
        )

        messages = [
            {"role": "system", "content": system_prompt},
            *conversation_history,
            {"role": "user", "content": user_message},
        ]
        if prefetched_tools:
            for idx, tool in enumerate(prefetched_tools, start=1):
                tool_call_id = f"prefetch_{idx}"
                messages.append({
                    "role": "assistant",
                    "tool_calls": [{
                        "id": tool_call_id,
                        "type": "function",
                        "function": {
                            "name": tool["name"],
                            "arguments": json.dumps(tool.get("arguments", {})),
                        },
                    }],
                })
                messages.append({
                    "role": "tool",
                    "tool_call_id": tool_call_id,
                    "content": json.dumps(tool.get("result", {})),
                })

        tool_calls_used = 0
        tool_definitions = tools or []

        while True:
            tool_call_name = None
            tool_call_id = None
            tool_args_json = ""

            stream = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                max_tokens=150,
                temperature=0.7,
                stream=True,
                tools=tool_definitions or None,
                tool_choice="auto",
            )

            async for chunk in stream:
                delta = chunk.choices[0].delta

                if getattr(delta, "tool_calls", None):
                    for call in delta.tool_calls:
                        if call.id:
                            tool_call_id = call.id
                        if call.function and call.function.name:
                            tool_call_name = call.function.name
                        if call.function and call.function.arguments:
                            tool_args_json += call.function.arguments
                    continue

                if delta.content:
                    if tool_call_name:
                        continue
                    yield {"type": "content", "text": delta.content}

            if tool_call_name:
                tool_calls_used += 1
                if tool_calls_used > max_tool_calls or not tool_executor:
                    yield {
                        "type": "content",
                        "text": "I'm having trouble pulling that up right now. Would you like me to take a message?",
                    }
                    break

                try:
                    tool_args = json.loads(tool_args_json or "{}")
                except json.JSONDecodeError:
                    tool_args = {}

                missing_info = self._validate_tool_args(tool_call_name, tool_args)
                if missing_info:
                    yield {"type": "content", "text": missing_info}
                    break

                yield {"type": "tool_call", "name": tool_call_name, "arguments": tool_args}

                tool_result = await tool_executor(tool_call_name, tool_args)

                tool_call_id = tool_call_id or f"tool_call_{tool_calls_used}"
                messages.append({
                    "role": "assistant",
                    "tool_calls": [{
                        "id": tool_call_id,
                        "type": "function",
                        "function": {
                            "name": tool_call_name,
                            "arguments": json.dumps(tool_args),
                        },
                    }],
                })
                messages.append({
                    "role": "tool",
                    "tool_call_id": tool_call_id,
                    "content": json.dumps(tool_result),
                })
                continue

            break

    def _validate_tool_args(self, tool_name: str, tool_args: dict) -> Optional[str]:
        """Validate tool args and return a user prompt if required fields are missing."""
        if tool_name in {"get_policies", "get_faqs"}:
            if not tool_args.get("topic"):
                return "Which topic should I check? For example: cancellation, pricing, or parking."
        if tool_name == "get_booking_by_id" and not tool_args.get("booking_id"):
            return "Do you have the booking ID?"
        if tool_name == "get_latest_booking" and not tool_args.get("customer_phone"):
            return "Can I grab the mobile number on the booking?"
        return None


# Singleton instance
streaming_ai_service = StreamingAIService()
