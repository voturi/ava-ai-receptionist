"""
Streaming AI Service for real-time LLM responses.

Uses OpenAI's streaming API to yield tokens as they're generated,
enabling low-latency text-to-speech synthesis.
"""

from __future__ import annotations

import os
from typing import Any, AsyncGenerator, Callable, Optional
import json

from openai import AsyncOpenAI


class StreamingAIService:
    """
    AI service with streaming token output.

    Designed for the streaming voice pipeline:
    STT transcript → LLM (streaming) → TTS (streaming)
    """

    def __init__(self):
        self.client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        # Use gpt-4o-mini for faster responses (good balance of speed/quality)
        self.model = "gpt-4o-mini"

    def get_system_prompt(
        self,
        business_name: str = "our business",
        business_config: Optional[dict] = None,
    ) -> str:
        """
        System prompt optimized for voice conversations.

        Key differences from text-based prompts:
        - Shorter responses (voice needs to be concise)
        - Natural speech patterns
        - No markdown or formatting
        """
        if business_config is None:
            business_config = {}

        ai_config = business_config.get("ai_config") or {}
        industry = business_config.get("industry") or "business"
        services = business_config.get("services") or []
        working_hours = business_config.get("working_hours") or {}

        tone = ai_config.get("tone", "warm, friendly, and professional")
        language = ai_config.get("language", "en-AU")

        services_summary = ", ".join(
            service.get("name", str(service)) if isinstance(service, dict) else str(service)
            for service in services[:8]
        )
        if not services_summary:
            services_summary = "Ask if the caller needs a service."

        working_hours_summary = ", ".join(
            f"{day}: {hours}" for day, hours in list(working_hours.items())[:7]
        )
        if not working_hours_summary:
            working_hours_summary = "Ask if the caller needs business hours."

        policies_summary = business_config.get("policies_summary") or "Not provided."
        faqs_summary = business_config.get("faqs_summary") or "Not provided."

        return f"""You are Echo, the AI receptionist for {business_name} ({industry}).
Tone: {tone}. Language: {language}. Be warm and concise (1-2 sentences).

BUSINESS CONTEXT:
- Services: {services_summary}
- Hours: {working_hours_summary}
- Policies: {policies_summary}
- FAQs: {faqs_summary}

TOOLS POLICY:
- Use tools only for booking lookups when explicitly asked.
- Booking lookups use caller phone (do not request business_id).

TRADIES BEHAVIOR:
- If urgent issue (burst pipe, flooding, gas smell, no power), ask for address + safety step, then offer urgent dispatch.
- Ask for job details: issue type, address/suburb, access notes, preferred time window.
- Keep responses short and reassuring.

BOOKING FLOW (mandatory fields):
1. Service needed
2. Preferred day
3. Preferred time
4. Customer name
5. Customer mobile number (required for confirmation)
6. Confirm all details before finalizing

IMPORTANT: Always collect the mobile number before confirming a booking.
If booking intent is detected, ask for missing fields in order and do not confirm until name and mobile are collected.
Once all details are collected, ask for explicit permission to finalize the booking (e.g., "Shall I go ahead and finalise that?") and wait for a yes.

VOICE CONVERSATION RULES:
- Use Australian expressions: "no worries", "lovely", "arvo"
- Keep responses SHORT: 1-2 sentences, 15-25 words max
- Sound natural and warm, like a friendly human
- Never use bullet points, lists, or formatted text
- Don't say "I'm an AI" - just be helpful
- Do NOT say goodbye unless the booking is confirmed or the request is fully resolved
- Avoid farewell language before confirmation; keep the conversation open-ended
- Do NOT claim a booking is confirmed; say you'll confirm once details are collected

If unsure about anything, say "Let me check on that for you" and keep it brief."""

    async def get_streaming_response(
        self,
        user_message: str,
        conversation_history: Optional[list] = None,
        business_name: str = "our business",
    ) -> AsyncGenerator[str, None]:
        """
        Stream AI response token by token.

        Yields text chunks as they're generated by the LLM.
        Perfect for piping directly to streaming TTS.

        Usage:
            async for chunk in ai.get_streaming_response("Hello"):
                await tts.send_text(chunk)
            await tts.flush()
        """
        if conversation_history is None:
            conversation_history = []

        messages = [
            {"role": "system", "content": self.get_system_prompt(business_name)},
            *conversation_history,
            {"role": "user", "content": user_message},
        ]

        try:
            stream = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                max_tokens=100,  # Keep responses short for voice
                temperature=0.7,
                stream=True,
            )

            async for chunk in stream:
                delta = chunk.choices[0].delta
                if delta.content:
                    yield delta.content

        except Exception as e:
            print(f"❌ OpenAI Streaming Error: {e}")
            # Yield a fallback message
            yield "Sorry, I'm having trouble right now. Can you try again?"

    async def get_response_with_buffer(
        self,
        user_message: str,
        conversation_history: Optional[list] = None,
        business_name: str = "our business",
        min_chunk_size: int = 10,
    ) -> AsyncGenerator[str, None]:
        """
        Stream AI response with buffering for natural speech breaks.

        Buffers tokens until we hit a natural break point (punctuation)
        or reach min_chunk_size. This produces better TTS output.

        Args:
            user_message: The user's input
            conversation_history: Previous messages
            business_name: Business name for personalization
            min_chunk_size: Minimum characters before yielding on punctuation
        """
        buffer = ""

        async for token in self.get_streaming_response(
            user_message,
            conversation_history,
            business_name,
        ):
            buffer += token

            # Check for natural break points
            if self._should_yield(buffer, min_chunk_size):
                yield buffer
                buffer = ""

        # Yield any remaining text
        if buffer:
            yield buffer

    def _should_yield(self, buffer: str, min_size: int) -> bool:
        """
        Determine if we should yield the buffer to TTS.

        Yields on:
        - Sentence endings (. ! ?)
        - Clause breaks (, ; :) if buffer is long enough
        - Buffer exceeds max size (50 chars)
        """
        if not buffer:
            return False

        stripped = buffer.rstrip()

        # Always yield on sentence end
        if stripped.endswith((".", "!", "?")):
            return len(buffer) >= min_size

        # Yield on clause breaks if buffer is decent size
        if stripped.endswith((",", ";", ":")):
            return len(buffer) >= min_size

        # Yield if buffer is getting too long
        return len(buffer) >= 50

    async def stream_with_tools(
        self,
        user_message: str,
        conversation_history: Optional[list] = None,
        business_profile: Optional[dict] = None,
        tools: Optional[list] = None,
        tool_executor: Optional[Callable[[str, dict], Any]] = None,
        max_tool_calls: int = 2,
        prefetched_tools: Optional[list[dict]] = None,
    ) -> AsyncGenerator[dict, None]:
        """
        Stream a response with tool calling.

        Yields events:
        - {"type": "content", "text": "..."}
        - {"type": "tool_call", "name": "...", "arguments": {...}}
        """
        if conversation_history is None:
            conversation_history = []

        system_prompt = self.get_system_prompt(
            business_name=(business_profile or {}).get("business_name", "our business"),
            business_config=business_profile or {},
        )

        messages = [
            {"role": "system", "content": system_prompt},
            *conversation_history,
            {"role": "user", "content": user_message},
        ]
        if prefetched_tools:
            for idx, tool in enumerate(prefetched_tools, start=1):
                tool_call_id = f"prefetch_{idx}"
                messages.append({
                    "role": "assistant",
                    "tool_calls": [{
                        "id": tool_call_id,
                        "type": "function",
                        "function": {
                            "name": tool["name"],
                            "arguments": json.dumps(tool.get("arguments", {})),
                        },
                    }],
                })
                messages.append({
                    "role": "tool",
                    "tool_call_id": tool_call_id,
                    "content": json.dumps(tool.get("result", {})),
                })

        tool_calls_used = 0
        tool_definitions = tools or []

        while True:
            tool_call_name = None
            tool_call_id = None
            tool_args_json = ""

            stream = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                max_tokens=150,
                temperature=0.7,
                stream=True,
                tools=tool_definitions or None,
                tool_choice="auto",
            )

            async for chunk in stream:
                delta = chunk.choices[0].delta

                if getattr(delta, "tool_calls", None):
                    for call in delta.tool_calls:
                        if call.id:
                            tool_call_id = call.id
                        if call.function and call.function.name:
                            tool_call_name = call.function.name
                        if call.function and call.function.arguments:
                            tool_args_json += call.function.arguments
                    continue

                if delta.content:
                    if tool_call_name:
                        continue
                    yield {"type": "content", "text": delta.content}

            if tool_call_name:
                tool_calls_used += 1
                if tool_calls_used > max_tool_calls or not tool_executor:
                    yield {
                        "type": "content",
                        "text": "I'm having trouble pulling that up right now. Would you like me to take a message?",
                    }
                    break

                try:
                    tool_args = json.loads(tool_args_json or "{}")
                except json.JSONDecodeError:
                    tool_args = {}

                missing_info = self._validate_tool_args(tool_call_name, tool_args)
                if missing_info:
                    yield {"type": "content", "text": missing_info}
                    break

                yield {"type": "tool_call", "name": tool_call_name, "arguments": tool_args}

                tool_result = await tool_executor(tool_call_name, tool_args)

                tool_call_id = tool_call_id or f"tool_call_{tool_calls_used}"
                messages.append({
                    "role": "assistant",
                    "tool_calls": [{
                        "id": tool_call_id,
                        "type": "function",
                        "function": {
                            "name": tool_call_name,
                            "arguments": json.dumps(tool_args),
                        },
                    }],
                })
                messages.append({
                    "role": "tool",
                    "tool_call_id": tool_call_id,
                    "content": json.dumps(tool_result),
                })
                continue

            break

    def _validate_tool_args(self, tool_name: str, tool_args: dict) -> Optional[str]:
        """Validate tool args and return a user prompt if required fields are missing."""
        if tool_name in {"get_policies", "get_faqs"}:
            if not tool_args.get("topic"):
                return "Which topic should I check? For example: cancellation, pricing, or parking."
        if tool_name == "get_booking_by_id" and not tool_args.get("booking_id"):
            return "Do you have the booking ID?"
        if tool_name == "get_latest_booking" and not tool_args.get("customer_phone"):
            return "Can I grab the mobile number on the booking?"
        return None


# Singleton instance
streaming_ai_service = StreamingAIService()
