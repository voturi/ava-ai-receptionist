"""
Streaming AI Service for real-time LLM responses.

Uses OpenAI's streaming API to yield tokens as they're generated,
enabling low-latency text-to-speech synthesis.
"""

from __future__ import annotations

import os
from typing import AsyncGenerator, Optional

from openai import AsyncOpenAI


class StreamingAIService:
    """
    AI service with streaming token output.

    Designed for the streaming voice pipeline:
    STT transcript → LLM (streaming) → TTS (streaming)
    """

    def __init__(self):
        self.client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        # Use gpt-4o-mini for faster responses (good balance of speed/quality)
        self.model = "gpt-4o-mini"

    def get_system_prompt(self, business_name: str = "our business") -> str:
        """
        System prompt optimized for voice conversations.

        Key differences from text-based prompts:
        - Shorter responses (voice needs to be concise)
        - Natural speech patterns
        - No markdown or formatting
        """
        return f"""You are Sarah, the AI receptionist for {business_name}.

VOICE CONVERSATION RULES:
- Keep responses SHORT: 1-2 sentences, 15-25 words max
- Sound natural and warm, like a friendly human
- Use Australian expressions: "no worries", "lovely", "arvo"
- Never use bullet points, lists, or formatted text
- Don't say "I'm an AI" - just be helpful

BOOKING FLOW (one step at a time, don't skip any):
1. What service do they need?
2. What day works for them?
3. What time?
4. What's their name?
5. What's their mobile number? (for SMS confirmation)
6. Repeat back ALL details and confirm

IMPORTANT: You MUST ask for their mobile number before confirming. Never skip this step.

EXAMPLE CONVERSATION:
Customer: "I need a haircut"
You: "No worries! When would suit you - this week or next?"
Customer: "Wednesday arvo"
You: "Lovely! What time works - 2, 3, or 4pm?"
Customer: "2pm"
You: "Perfect! And your name?"
Customer: "Sarah"
You: "Great Sarah! What's your mobile for the SMS confirmation?"
Customer: "0412 345 678"
You: "Brilliant! So that's a haircut Wednesday 2pm for Sarah. I'll SMS you at 0412 345 678. All sorted!"

ENDING THE CALL:
- When customer says "thank you", "thanks", "bye", "that's all", or similar after booking is confirmed
- Say a brief friendly goodbye like "No worries! See you Wednesday. Bye!"
- End with the word "Goodbye" to signal call end

If unsure about anything, say "Let me check on that for you" and keep it brief."""

    async def get_streaming_response(
        self,
        user_message: str,
        conversation_history: Optional[list] = None,
        business_name: str = "our business",
    ) -> AsyncGenerator[str, None]:
        """
        Stream AI response token by token.

        Yields text chunks as they're generated by the LLM.
        Perfect for piping directly to streaming TTS.

        Usage:
            async for chunk in ai.get_streaming_response("Hello"):
                await tts.send_text(chunk)
            await tts.flush()
        """
        if conversation_history is None:
            conversation_history = []

        messages = [
            {"role": "system", "content": self.get_system_prompt(business_name)},
            *conversation_history,
            {"role": "user", "content": user_message},
        ]

        try:
            stream = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                max_tokens=100,  # Keep responses short for voice
                temperature=0.7,
                stream=True,
            )

            async for chunk in stream:
                delta = chunk.choices[0].delta
                if delta.content:
                    yield delta.content

        except Exception as e:
            print(f"❌ OpenAI Streaming Error: {e}")
            # Yield a fallback message
            yield "Sorry, I'm having trouble right now. Can you try again?"

    async def get_response_with_buffer(
        self,
        user_message: str,
        conversation_history: Optional[list] = None,
        business_name: str = "our business",
        min_chunk_size: int = 10,
    ) -> AsyncGenerator[str, None]:
        """
        Stream AI response with buffering for natural speech breaks.

        Buffers tokens until we hit a natural break point (punctuation)
        or reach min_chunk_size. This produces better TTS output.

        Args:
            user_message: The user's input
            conversation_history: Previous messages
            business_name: Business name for personalization
            min_chunk_size: Minimum characters before yielding on punctuation
        """
        buffer = ""

        async for token in self.get_streaming_response(
            user_message,
            conversation_history,
            business_name,
        ):
            buffer += token

            # Check for natural break points
            if self._should_yield(buffer, min_chunk_size):
                yield buffer
                buffer = ""

        # Yield any remaining text
        if buffer:
            yield buffer

    def _should_yield(self, buffer: str, min_size: int) -> bool:
        """
        Determine if we should yield the buffer to TTS.

        Yields on:
        - Sentence endings (. ! ?)
        - Clause breaks (, ; :) if buffer is long enough
        - Buffer exceeds max size (50 chars)
        """
        if not buffer:
            return False

        stripped = buffer.rstrip()

        # Always yield on sentence end
        if stripped.endswith((".", "!", "?")):
            return len(buffer) >= min_size

        # Yield on clause breaks if buffer is decent size
        if stripped.endswith((",", ";", ":")):
            return len(buffer) >= min_size

        # Yield if buffer is getting too long
        return len(buffer) >= 50


# Singleton instance
streaming_ai_service = StreamingAIService()
